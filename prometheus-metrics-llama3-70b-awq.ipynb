{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a399823e",
   "metadata": {},
   "source": [
    "## Prometheus Metrics\n",
    "- Reference: https://nvidia.github.io/TensorRT-LLM/examples/prometheus_metrics.html\n",
    "\n",
    "- Modified script for smoke tests measurement via Prometheus metrics\n",
    "\n",
    "```\n",
    "podman ps\n",
    "CONTAINER ID  IMAGE                              COMMAND               CREATED        STATUS        PORTS       NAMES\n",
    "2c0e74b44b1e  docker.io/vllm/vllm-openai:latest  -lc vllm serve /m...  7 minutes ago  Up 7 minutes              brave_wozniak\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -s http://localhost:8002/v1/models | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6911ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -s http://localhost:8002/metrics | grep -E '^(vllm|trtllm|# HELP|# TYPE)' | head -n 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8002/v1\",\n",
    "    api_key=\"tensorrt_llm\",\n",
    ")\n",
    "\n",
    "# Prometheus metric prefix used by TensorRT-LLM\n",
    "METRIC_PREFIX = \"vllm:\"\n",
    "\n",
    "# Base URL for the metrics endpoint\n",
    "METRICS_URL = \"http://localhost:8002/metrics\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metrics() -> dict | None:\n",
    "    \"\"\"Fetch metrics from the Prometheus endpoint.\"\"\"\n",
    "    try:\n",
    "        response = urlopen(METRICS_URL)\n",
    "        if response.status == 200:\n",
    "            return response.read().decode(\"utf-8\")\n",
    "        else:\n",
    "            print(f\"Error fetching metrics: HTTP {response.status}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metrics: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_display_metrics(metrics_data: dict) -> None:\n",
    "    \"\"\"Parse and display relevant TensorRT-LLM metrics.\"\"\"\n",
    "    if not metrics_data:\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TensorRT-LLM Prometheus Metrics\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define metrics to display with descriptions\n",
    "    metrics_of_interest = {\n",
    "        f\"{METRIC_PREFIX}request_success_total\": \"Total successful requests\",\n",
    "        f\"{METRIC_PREFIX}e2e_request_latency_seconds\": \"End-to-end request latency\",\n",
    "        f\"{METRIC_PREFIX}time_to_first_token_seconds\": \"Time to first token\",\n",
    "        f\"{METRIC_PREFIX}request_queue_time_seconds\": \"Request queue time\",\n",
    "        f\"{METRIC_PREFIX}kv_cache_hit_rate\": \"KV cache hit rate\",\n",
    "        f\"{METRIC_PREFIX}kv_cache_utilization\": \"KV cache utilization\",\n",
    "        f\"{METRIC_PREFIX}prefix_cache_queries_total\": \"Prefix cache queries (tokens)\",\n",
    "        f\"{METRIC_PREFIX}prefix_cache_hits_total\": \"Prefix cache hits (cached tokens)\",\n",
    "        f\"{METRIC_PREFIX}external_prefix_cache_queries_total\": \"External prefix cache queries\",\n",
    "        f\"{METRIC_PREFIX}external_prefix_cache_hits_total\": \"External prefix cache hits\",\n",
    "\n",
    "    }\n",
    "\n",
    "    found_metrics = []\n",
    "    missing_metrics = []\n",
    "\n",
    "    for metric_name, description in metrics_of_interest.items():\n",
    "        if metric_name in metrics_data:\n",
    "            found_metrics.append((metric_name, description))\n",
    "        else:\n",
    "            missing_metrics.append((metric_name, description))\n",
    "\n",
    "    # Display found metrics\n",
    "    if found_metrics:\n",
    "        print(\"\\n✓ Available Metrics:\")\n",
    "        print(\"-\" * 80)\n",
    "        for metric_name, description in found_metrics:\n",
    "            # Extract the metric lines from the data\n",
    "            lines = [\n",
    "                line\n",
    "                for line in metrics_data.split(\"\\n\")\n",
    "                if line.startswith(metric_name) and not line.startswith(\"#\")\n",
    "            ]\n",
    "            print(f\"\\n{description} ({metric_name}):\")\n",
    "            for line in lines:\n",
    "                print(f\"  {line}\")\n",
    "\n",
    "    # Display missing metrics\n",
    "    if missing_metrics:\n",
    "        print(\"\\n✗ Not Yet Available:\")\n",
    "        print(\"-\" * 80)\n",
    "        for metric_name, description in missing_metrics:\n",
    "            print(f\"  {description} ({metric_name})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Prometheus Metrics Example\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"This script will:\")\n",
    "    print(\"1. Send several completion requests to a running TensorRT-LLM server\")\n",
    "    print(\n",
    "        \"2. After each response, fetch and display Prometheus metrics from the /prometheus/metrics endpoint\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Auto-detect first available model from the OpenAI-compatible /v1/models endpoint\n",
    "    model_id = client.models.list().data[0].id\n",
    "\n",
    "    # Make several completion requests to generate metrics\n",
    "    print(\"Sending completion requests...\")\n",
    "    num_requests = 10\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=model_id,\n",
    "                prompt=(\n",
    "                    f\"Hello, this is request {i + 1}. \"\n",
    "                    \"Use your greatest imagination in this request. Tell me a lot about\"\n",
    "                ),\n",
    "                max_tokens=1000,\n",
    "                stream=False,\n",
    "            )\n",
    "            print(\n",
    "                f\"  Request {i + 1}/{num_requests} completed. Response: {response.choices[0].text[:50]}...\"\n",
    "            )\n",
    "\n",
    "            # Fetch and display metrics after each response\n",
    "            print(f\"\\n  Fetching metrics after request {i + 1}...\")\n",
    "            metrics_data = fetch_metrics()\n",
    "            if metrics_data:\n",
    "                parse_and_display_metrics(metrics_data)\n",
    "            else:\n",
    "                print(\"  ✗ Failed to fetch metrics\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"  Error on request {i + 1}: {e}\")\n",
    "    print(\"All requests completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f90b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab6018",
   "metadata": {},
   "source": [
    "#### TP2\n",
    "\n",
    "```\n",
    "rteixeira@elita:/mnt/elita/soundwave/playground$ nvidia-smi\n",
    "Sun Mar  1 00:21:17 2026\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 580.119.02             Driver Version: 580.119.02     CUDA Version: 13.0     |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |\n",
    "| N/A   40C    P0             82W /  350W |   43147MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |\n",
    "| N/A   40C    P0             86W /  350W |   43147MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   2  NVIDIA L40S                    On  |   00000000:CA:00.0 Off |                    0 |\n",
    "| N/A   31C    P8             32W /  350W |       0MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "|   3  NVIDIA L40S                    On  |   00000000:E1:00.0 Off |                    0 |\n",
    "| N/A   30C    P8             32W /  350W |       0MiB /  46068MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A          420305      C   VLLM::Worker_TP0                      43138MiB |\n",
    "|    1   N/A  N/A          420306      C   VLLM::Worker_TP1                      43138MiB |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a4533",
   "metadata": {},
   "source": [
    "### On container:\n",
    "\n",
    "```bash\n",
    " podman run --rm -it --net=host   --hooks-dir=/usr/share/containers/oci/hooks.d   --security-opt=label=disable   --device nvidia.com/gpu=0   --device nvidia.com/gpu=1   -e HF_HOME=/mnt/elita/soundwave/hf_cache   -e VLLM_CACHE_DIR=/mnt/elita/soundwave/vllm_cache   -v /mnt/elita/soundwave:/mnt/elita/soundwave   --entrypoint /bin/bash   docker.io/vllm/vllm-openai:latest -lc   'vllm serve /mnt/elita/soundwave/models/llama3-70b-awq \\\n",
    "     --port 8002 \\\n",
    "     --tensor-parallel-size 2 \\\n",
    "     2>&1 | tee /mnt/elita/soundwave/logs/vllm_tp2_default_DEBUG_8002.log'\n",
    "```\n",
    "\n",
    "```\n",
    "<...>\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
